# app/services/llm.py
import os
import json
import logging
import asyncio
import datetime
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv
import httpx
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from app.core.config import settings
from app.core.rabbitmq import mq
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ—Ç –ø—É—Ç—å –±—É–¥–µ—Ç —Ç–∞–∫–∏–º (—Ä–µ–∞–ª–∏–∑—É–µ–º –≤ —Å–ª–µ–¥. —Ñ–∞–π–ª–µ)
from app.utils.redis_lock import DistributedSemaphore, close_redis

load_dotenv()
logger = logging.getLogger("llm_service")

from app.core.config import settings



SMART_MODEL=settings.llm.smart_model,
MAIN_MODEL=settings.llm.main_model,
MAX_TOKENS=settings.llm.max_tokens,
TEMPERATURE=settings.llm.temperature
request_timeout=settings.llm.request_timeout


# –õ–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å Rate Limit –æ—Ç OpenAI)
GLOBAL_LLM_LIMIT = int(os.getenv("GLOBAL_LLM_CONCURRENCY", 30))

# --- –ù–ê–°–¢–†–û–ô–ö–ê –ü–†–û–ö–°–ò ---
SQUID_HOST = os.getenv("SQUID_PROXY_HOST")
SQUID_PORT = os.getenv("SQUID_PROXY_PORT")
SQUID_USER = os.getenv("SQUID_PROXY_USER")
SQUID_PASS = os.getenv("SQUID_PROXY_PASSWORD")

proxy_url = None
if SQUID_HOST:
    proxy_url = f"http://{SQUID_USER}:{SQUID_PASS}@{SQUID_HOST}:{SQUID_PORT}"

async_http_client = httpx.AsyncClient(
    proxy=proxy_url,
    timeout=request_timeout
)

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    http_client=async_http_client
)

if proxy_url:
    logger.info(f"üåê OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏: {SQUID_HOST}:{SQUID_PORT}")



# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

async def send_llm_alert(error_type: str, error_msg: str, diag_id: Any):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º —Å–±–æ–µ LLM"""
    try:
        payload = {
            "type": "system",
            "text": f"üö® **LLM CRITICAL ERROR**\n\n**–¢–∏–ø:** {error_type}\n**–î–∏–∞–ª–æ–≥:** `{diag_id}`\n**–û—à–∏–±–∫–∞:** {error_msg}",
            "alert_type": "admin_only"
        }
        await mq.publish("tg_alerts", payload)
    except:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞–ª–µ—Ä—Ç –æ —Å–±–æ–µ LLM")

def calculate_usage(usage, model_name: str) -> Dict[str, Any]:
    """–î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∫—ç—à–∞ (–∫–∞–∫ –≤ HH –±–æ—Ç–µ)"""
    cached_tokens = 0
    if hasattr(usage, "prompt_tokens_details") and usage.prompt_tokens_details:
        cached_tokens = getattr(usage.prompt_tokens_details, "cached_tokens", 0)
    
    prompt_tokens = usage.prompt_tokens
    cache_pc = (cached_tokens / prompt_tokens * 100) if prompt_tokens > 0 else 0
    
    return {
        "prompt_tokens": prompt_tokens,
        "completion_tokens": usage.completion_tokens,
        "total_tokens": usage.total_tokens,
        "cached_tokens": cached_tokens,
        "cache_percentage": round(cache_pc, 1),
        "model": model_name
    }


# --- –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ ---

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)
async def get_bot_response(
    system_prompt: str, 
    dialogue_history: List[Dict], 
    user_message: str, 
    extra_context: Optional[Dict] = None,
    attempt_tracker: Optional[List] = None
) -> Dict[str, Any]:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–∏–∞–ª–æ–≥–∞. 
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ù–´–ô –°–ï–ú–ê–§–û–† –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ª–∏–º–∏—Ç–æ–≤ –º–µ–∂–¥—É –≤–æ—Ä–∫–µ—Ä–∞–º–∏.
    """
    if attempt_tracker is not None:
        attempt_tracker.append(datetime.datetime.now())

    diag_id = (extra_context or {}).get("dialogue_id", "unknown")
    ctx_logger = logging.LoggerAdapter(logger, extra_context or {})
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è (–≤–∞–∂–Ω–æ –¥–ª—è –¥–∞—Ç)
    now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    full_system_prompt = f"{system_prompt}\n\nCurrent time: {now_str}"

    messages = [{"role": "system", "content": full_system_prompt}]
    messages.extend(dialogue_history)
    messages.append({"role": "user", "content": user_message})

    try:
        ctx_logger.info(f"üß¨üß¨üß¨ [Action: llm_request_start] Model: {MAIN_MODEL}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Redis-—Å–µ–º–∞—Ñ–æ—Ä "llm_global"
        async with DistributedSemaphore(name="llm_global", limit=GLOBAL_LLM_LIMIT):
            response = await client.chat.completions.create(
                model=MAIN_MODEL,
                messages=messages,
                max_completion_tokens=MAX_TOKENS,
                response_format={"type": "json_object"},
                frequency_penalty=0.7,
                temperature=TEMPERATURE
            )

        content = response.choices[0].message.content
        stats = calculate_usage(response.usage, MAIN_MODEL)

        ctx_logger.info(
            f"‚úÖ LLM Response. [Action: llm_response_success] "
            f"Tokens: {stats['total_tokens']} (Cached: {stats['cache_percentage']}%)"
        )

        return {
            "parsed_response": json.loads(content),
            "usage_stats": stats
        }

    except Exception as e:
        error_name = type(e).__name__
        ctx_logger.warning(f"‚ö†Ô∏è [Action: llm_request_retry] Error: {error_name}: {e}")
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ (retry –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è 3 —Ä–∞–∑–∞)
        # Tenacity –≤—ã–±—Ä–æ—Å–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ, –Ω–æ –º—ã —É—Å–ø–µ–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞–ª–µ—Ä—Ç
        if attempt_tracker and len(attempt_tracker) >= 3:
            await send_llm_alert(f"Final Retry Failed ({MAIN_MODEL})", str(e), diag_id)
            
        raise

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)
async def get_smart_bot_response(
    system_prompt: str, 
    dialogue_history: List[Dict], 
    user_message: str, 
    extra_context: Optional[Dict] = None,
    attempt_tracker: Optional[List] = None
) -> Dict[str, Any]:
    """
    –ú–µ—Ç–æ–¥ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (gpt-4o).
    """
    if attempt_tracker is not None:
        attempt_tracker.append(datetime.datetime.now())

    diag_id = (extra_context or {}).get("dialogue_id", "unknown")
    ctx_logger = logging.LoggerAdapter(logger, extra_context or {})
    
    now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # –î–ª—è –º–æ–¥–µ–ª–µ–π o1/gpt-4o —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–æ–ª—å developer
    messages = [
        {"role": "developer", "content": f"{system_prompt}\nContext time: {now_str}"}
    ]
    messages.extend(dialogue_history)
    messages.append({"role": "user", "content": user_message})

    try:
        ctx_logger.info(f"üß†üß† [Action: smart_llm_start] Model: {SMART_MODEL}")

        async with DistributedSemaphore(name="llm_global", limit=GLOBAL_LLM_LIMIT):
            response = await client.chat.completions.create(
                model=SMART_MODEL,
                messages=messages,
                max_completion_tokens=MAX_TOKENS,
                response_format={"type": "json_object"},
                temperature=TEMPERATURE
            )

        content = response.choices[0].message.content
        stats = calculate_usage(response.usage, SMART_MODEL)

        ctx_logger.info(
            f"‚úÖ SMART Response. [Action: smart_llm_success] "
            f"Tokens: {stats['total_tokens']} (Cached: {stats['cache_percentage']}%)"
        )

        return {
            "parsed_response": json.loads(content),
            "usage_stats": stats
        }

    except Exception as e:
        ctx_logger.warning(f"‚ö†Ô∏è [Action: smart_llm_retry] Error: {e}")
        if attempt_tracker and len(attempt_tracker) >= 3:
            await send_llm_alert(f"Final Smart Retry Failed", str(e), diag_id)
        raise

async def cleanup_llm():
    """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã–∫–ª—é—á–µ–Ω–∏–∏"""
    await async_http_client.aclose()
    await close_redis()
    logger.info("üîí [Action: llm_cleanup] HTTP client and Redis connections closed")